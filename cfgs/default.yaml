defaults:
  - _self_
  - task: cheetah_run

  # disable hydra logging
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

seed: -1 # generate random seed if seed == -1 (by defalut)

env_name: ${env_name}
algo_name: tcrl
exp_name: default

# training
device: ???
obs_shape: ???
action_shape: ???
train_step: ???
action_repeat: 2

episode_length: 1000
train_episode: ${train_episode}
random_episode: 10
eval_interval: 25
eval_episode: 10
update_every_steps: 2

batch_size: 512
reward_coef: 1.0 # 20 to match model magnitude
state_coef: 1.0
horizon: 5
tau: 0.005
gamma: 0.99
rho: 0.9
grad_clip_norm: 10.
nstep: 3
buffer_num_workers: 4

lr: 3e-4
weight_decay: 1e-6 # only used when updating encoder and latent dynamics

mlp_dims: [ 512, 512 ]
latent_dim: ${latent_dim}

std_clip: 0.3
std_schedule: ${std_schedule}

# distraction
use_distraction: false
distraction_dimensions: 10
distraction_pure_noise: false
distraction_switching: true
distraction_correlated: false
distraction_linear: false
distraction_random_walk: true
distraction_reward_noise: true
distraction_obs_noise: true

value_expansion: td-k # td-k (tcrl), mve, double-mve, lambda-mve, joint
value_aggregation: min # min (tcrl), mean, max
normalize_z: false # tcrl=false
lambda_: 0.95
critic_mode: q # q (tcrl), value, double_model_value
critic_model_grad: none # none, first, both
model_loss: cosine # cosine (tcrl), mse

policy_update: ddpg # ddpg (tcrl), backprop, lambda_bp

# options
save_video: false
use_wandb: true
save_buffer: false
save_model: false
save_interval: 500
save_logging: false

run_suffix: 0
# ref: https://stackoverflow.com/questions/65104134/disable-file-output-of-hydra
hydra:
  output_subdir: null
  run:
    dir: .
